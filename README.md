# 引言

> 2025-06-05
> 先凑合一下

## 要解决的问题
频谱感知的工作，简单来说，即在时频域上实现将噪声背景上的信号迹线分选出来。

例如原始IQ数据经过FFT变换后得到其时域上的功率谱密度，这里我们将其存入一个逗号分隔符文件.csv
这个文件的格式可能如下所示：

time,freq_1,freq_2,...

2024-05-06 11:00:00,-2,-2.1,...

2024-05-06 11:00:01,-2.1,-2.2,...

其中第一列为时间，其余列为频率采样点对应时间的功率值。

这个.csv文件可以被绘制为瀑布图，我们需要将这个瀑布图上的信号迹线“框选”出来。

受到YOLO中处理图片的方法的启发，YOLO中似乎是将放缩后的图片划分为相同大小的小方格，然后每
个小方格里有数个检测框，进行识别，然后每个检测框有一个置信度之类的指标，通过置信度筛选掉那
些置信度比较低的框，并通过非极大值抑制之类的方法合并重叠的框，从而得到我们看到的YOLO的检测
框。

现在想到一个思路是也把频谱数据分割成一个个小块，然后通过对比学习之类的方法为可能的信号迹线
标记锚框，然后再合并可能重叠的锚框，通过聚类之类的方法处理，最终得到框选的信号迹线。
首先要把宽频段的频谱数据进行处理，比如也把它们分割成128*600的小块，代表128个频率采样点在
600s上的能量值变化。

对于这个128*600的区域需要多少个锚框呢？一开始并不知道这个区域有多少条信号迹线（可能是1条或
者10条），就为这个区域设置20个锚框。

目前的思路是通过这个锚框的中心点的频点和时间点、这个锚框的频带宽度（宽度）和时间跨度（高
度）来标定这个锚框的位置和大小，即[center_freq,center_time,width,height]。

接下来是初始化锚框，刚开始我们并不知道信号迹线的位置，所以我们只能随机分配锚框的位置和大
小，例如随机生成20个[center_freq,center_time,width,height]，后面再慢慢调整。

但这里存在一个问题，如果锚框是完全随机的话，那这个框很有可能落在漆黑的噪声区域里（毕竟我们
假设这个跳频信号频谱上的信号迹线是稀疏的），可能会导致计算量大且效率低（需要很多次调整才可
能挨着信号区域的边）。

所以这里可以采用启发式的方法，比如在生成锚框时，选择那些能量高的点作为锚框的中心点。
比如可以先对频谱数据的所有点进行排序，选择能量值最高的前500个点，从这500个点里随机挑20个点
作为锚框的中心点（也就是center_freq,center_time）。这样的话初始化的锚框很有可能本身就落在信
号区域上了。（这也是为什么选择锚框中心点而不是左上点进行定位）

锚框的长宽在初始化时也是随机生成的，但一般信号迹线是扁平的，因此在生成时可以按照一定的比例
生成，比如在生成时规定width:height=10:1左右。

接下来是对锚框进行调整，

锚框调整有两个关键点：

一是如何判断这个锚框“好不好”，也就是给这个锚框进行评分，有点类似于YOLO的目标检测框的置信
度。

锚框评分的本质是评估锚框内区域包含信号迹线的概率，也就是评价它有“多大的可能是一个信号的迹
线”。比如说，如果一个框里边全是噪声，那它的评分就会非常低，如果一个框里边有一半是信号，有一
半是噪声，那它的评分会高一点，需要往评分更高的方向（比如往信号区域平移或扩展）。
如果一个框里边全是信号区域，那它的评分会很高，但是：如果它的周围也是信号区域呢？这个锚框应
该继续扩大才对，所以这个评分机制要考虑到这种情况，不能仅仅依靠框内信号的占比给这个锚框评
分。

二是如何调整这个锚框，比如说我们初始化的锚框的评分很低，如何去调整它呢？应该往哪个方向平
移、收缩、扩展这个锚框呢？

如果直接手动实现这个调整逻辑，那这个处理逻辑会非常繁琐，而且也没啥创新。
但考虑到每个锚框有评分，所以现在在思考是不是可以用一个强化学习的模型去决定如何调整锚框，毕
竟感觉评分机制挺适合强化学习的。

## 强化学习的启发

一个典型的强化学习逻辑（强化学习的核心思想是让智能体在试错（trial-and-error）过程中不断调整策
略，最终学会最优策略。智能体（Agent）通过与环境（Environment）交互，学习如何采取行动以最大
化某种累积奖励。）：

可以把它抽象为这样一个马尔可夫决策过程：

- 状态（state）：当前锚框状态+局部或全局频谱环境特征。
> 也就是锚框自身的状态，以及锚框周围的一大圈数据（局部），或者所有数据（全局），这一大堆
东西视为状态
- 动作（Action）：对锚框参数的调整操作（平移、收缩、扩展？）
- 奖励（Reward）：评估动作效果的反馈，
- 策略（policy）：神经网络，根据状态决定最佳动作。

这是一个大概的框架，强化学习模型根据当前锚框状态去调整锚框，然后根据调整后的评分给予模型反
馈，也就是奖励函数的作用，大致通过这样一个流程让模型学习如何将锚框调整为最能“框”住一条信号迹
线的状态。

强化学习的思路关键是，如何设计一个好的奖励函数让它能更好地、更有效率地调整。

如果这个方法可行的话，那可以生成一堆信号迹线的坐标：

```
detect_box_list=[[center_freq,center_time,width,height],
 [center_freq,center_time,width,height],
 [center_freq,center_time,width,height],
 [center_freq,center_time,width,height],
 ......
 [center_freq,center_time,width,height],
 [center_freq,center_time,width,height],
 [center_freq,center_time,width,height]]
```

然后以每条迹线坐标对应的迹线为处理的基本单位，结合其他数据（比如根据迹线坐标进一步提取迹线
里的更细粒度的数据），输入时序模型中，进行分析或者异常检测什么的。

# 方法论：在开始写代码前对强化学习的思考

接之前的强化学习进行锚框调整的思路，发现还是想得有点太简单了。

## 关于如何定义锚框的“状态”

一开始我以为，定义锚框的状态很简单，直接把锚框区域内的数据和锚框区域外的数据直接作为状态不就好了吗，比如现在我有一个锚框[center_freq,center_time,width,height]，还有全部的时频能量矩阵Dataframe df，那么我只需要通过锚框坐标得到锚框内的能量矩阵df_inner和锚框周围一片区域的能量矩阵df_around不就好了吗？把它们直接整合成状态向量。

但是发现这个思路似乎不行，存在两个问题：
- 因为假设这个锚框大小为120×30，那么就包含3600格数据，再算上锚框周围的数据，假设是160×70（也就是锚框横竖扩充40格长度），那么这个状态向量的维度会非常大，而且状态向量是一维的，不能直接把二维的矩阵数据当成状态向量。
- 锚框的大小是不固定的，但“状态” 可能需要一个固定维度的输入。

那么接下来有两种思路，一种是把锚框内和周边的数据输入到一个CNN里边，输出一个固定维度的向量（比如64维或者128维），这个向量可以作为反映当前状态的状态向量。
对于这个思路也有存疑的地方，输出向量的维度数有什么影响呢？比如输出64维的状态向量与128维的状态向量，对于后续的强化学习来说有什么区别呢？维度数量是越大越好吗？

另一种思路是对锚框内和周边的原始数据进行一个类似于“池化”的操作，相当于把一张图片压缩成固定尺寸的图片。或者换句话说把df压缩成特定shape的df，不过这个思路不太完善，因为如前所述，状态向量是一维的，依然不能直接把二维的矩阵数据当成状态向量。

一开始采取的思路是把锚框内和周边的数据输入到一个CNN里边，输出一个固定维度的向量，但这里也存在着一个问题，
特征提取模型（如CNN）本质上是一个函数映射器，其权重决定了输入数据如何被转换为特征向量。
未训练的模型权重是随机初始化的，这些随机权重无法捕捉数据中的模式，因此提取的特征没有实际意义。
例如，一个未训练的CNN无法区分信号区域和噪声区域，提取的特征无法反映锚框内是否包含信号迹线。

所以我们还需要先训练这个CNN，让它学会区分信号区域和噪声区域，从而学习到对强化学习任务有意义的特征。

我最开始思考了这样一种方法，虽然我无法标记数据，但是我可以通过生成噪声锚框和信号锚框的方法生成大致的噪声和信号形态 ，
比如说吧，我可以随机生成一些锚框，假如50个锚框的中心点是在能量值最低的500个点里随机取的，那么这些框大概率就是噪声框，
如果50个锚框的中心点是在能量值最高的500个点里随机取的，那么这些框里大概率存在着信号（虽然也可能包含噪声）

用这种方法，我在训练CNN模型的时候可以传入一个df_noise_list和df_signal_list，
其中df_noise_list是噪声锚框内的区域的局部df组成的列表，然后我们可以把它们输入到CNN里进行训练，
让CNN学会区分噪声框和信号框，

也就是说，我们相当于去训练一个二分类的模型，在训练过程中，模型也学会了如何提取区域特征。
然后在生成特征向量时，让模型直接输出经过卷积层和池化层后的结果（相当于输出整个二分类模型的中间结果）

结果表明......这个方法效果不好，只能作为一个idea储备了，最终采取的是更简单点的方法，也就是第二种方法，直接把df矩阵压缩或拉伸成8*8的矩阵，类似于图片压缩的思路。

> 这个CNN最主要的问题在于训练上。
>
> 我一开始想的是生成噪声锚框和信号锚框，让CNN能去学到区分这两种锚框的方法，从而使它的卷积层和池化层能提取到比较复杂的特征。
>
> 但这个方法有个比较抽象的漏洞：网络为啥要学到信号边缘之类的特征呢？它完全可以根据整体能量的高低判断是否是信号锚框，所以中间层的特征向量还真不一定能反应出信号的特征来，它可能只是发现这个锚框里整体的能量比较高罢了。

### 注意力机制
假设我们已经通过CNN等乱七八糟的方法生成了一个64维的状态向量，然后可以把这个锚框的[center_freq,center_time,width,height]四个属性连接在状态向量后面，形成一个68维的状态向量，作为最终的状态向量。

但这样也存在一个问题，如果我们直接把这个68维的状态向量直接简单粗暴地作为整体的“状态”，那么模型并不能意识到后四位的数值与前64位有什么区别，相当于这后四位的重要性被“稀释”了。

我的思路是在后续定义模型时引入注意力机制，需要让模型“注意”到这后四位与前64位不一样。


## 关于如何定义调整锚框的“动作”

这里锚框的“状态”还要考虑调整锚框的“动作”，既然采取了提取特征，然后转化为状态向量的方法，那么“动作”应该只能以块的尺寸为单位进行调整，比如左侧的边往左扩展一列的块，或者右侧的边往左收缩一列的块，然后生成新的锚框。

那么，当进行一次锚框调整之后，锚框里的块应该被划分地更细致一些，从而更细粒度地调整锚框的尺寸。如此一次次调整，直到块被划分得只有一格大小，这时候就可以认为这个锚框调整到头了。

但是锚框的“动作”要怎么定义呢？它是只能调整一格块的距离吗？还是可以移动多格呢？
对模型来说，应该让它输出一个离散的决策，比如向某个方向移动x格，x等于一个固定值。
还是让它输出一个连续的决策，比如向某个方向移动x格，其中x∈[0,m]呢？

例如以下代码
```
DISCRETE_ACTIONS = [
    {"type": "move", "param": "up"},
    {"type": "move", "param": "down"},
    {"type": "move", "param": "left"},
    {"type": "move", "param": "right"},
    {"type": "resize_width", "param": "shrink"},
    {"type": "resize_width", "param": "expand"},
    {"type": "resize_height", "param": "shrink"},
    {"type": "resize_height", "param": "expand"}
]
```
就是给它定义了一个离散的动作空间。

# 奖励函数

这里需要明确一下训练的意义，首先是评分吧，我们需要给每个锚框一个评分，评价这个框的“好坏”，通过奖励函数鼓励模型往高评分的方向调整。

那么训练的意义是什么呢？我的理解是，通过让模型进行训练，模型可以逐步掌握如何最快地达到高评分，也就是实现根据“状态”，快速地、准确地通过“动作”调整锚框使其框选到信号迹线，模型学习到的东西就叫做策略。

因此奖励函数是至关重要的一环。

## 奖励函数权重的自动训练机制
一开始，对于奖励函数的设计是手动设置权重，比如有A,B两项，A项鼓励锚框保证锚框内的信号质量，B项鼓励锚框向外扩展，
如果A项权重过大，那么它可能会直接缩在信号区域的极大值上，而不是扩张到整个区域。
如果B项权重过大，那么它可能会直接扩张到噪声区域上，甚至无限扩大。

目前系统已经不再使用固定权重的评分函数，而是采用可学习的加权奖励函数机制。
具体来说，系统将多个不同的奖励分量（如区域内信号强度提升、锚框面积增长、边缘对比度、信号连通性、形状惩罚等）组合成一个总体奖励值。

> 也就是在训练中让模型自己去学到一个合适的奖励函数，为各项分配权重。
> 原来的奖励函数会作为一个可学习的线性组合，例如：
> 
> ``` reward = w1 * delta_score + w2 * area_gain ```
>
> 然后让这两个权重w1,w2本身成为一个可学习的参数.
> 
> 这一部分其实比较抽象，花了挺长时间去理解它，这个自动训练机制最主要的问题在于：“它要如何调整这些加权项呢？它又没有专家轨迹可供学习，甚至不知道怎么调整才是好的（毕竟就连reward函数本身都不确定）”


### 设定回顾：我们要解决什么？

我们之前的 reward 是显式加权的多项式：

$$
\text{reward} = w_1 \cdot \Delta \text{score} + w_2 \cdot \text{area\_gain}
$$

之前是手动试 $w_2 = 0.008$ 这种值。但这很难平衡，这个参数小了大了都不行，而且还得手动吭哧吭哧去试 —— 所以我们希望：

* **让 $w_1, w_2$ 变成神经网络中的可学习参数**；
* 让 PPO 的策略优化目标自动学习这些权重。

> 在标准的强化学习中：
>
> - 策略 $\pi_\theta(a|s)$ 的目标是最大化 reward 的期望：
>
>   $$ J(\theta) = \mathbb{E}_{\pi_\theta}[r(s, a)]$$
>
> - 这个 $r(s, a)$是环境给出的固定函数，不参与优化；
>
> - 所以训练时我们只更新 $\theta$，试图最大化当前定义下的 reward。
>
> 现在我们要让 reward 函数本身也变成一个可学习函数：
>
> - 设 reward 是由一些参数控制的，比如：
>
>   $$ r_w(s, a) = w_1 \cdot r_1(s,a) + w_2 \cdot r_2(s,a)$$
>
> 那么目标函数就变成联合优化：
>
> $$ J(\theta, w) = \mathbb{E}_{\pi_\theta}[r_w(s, a)] $$
>
> 我们现在不仅要训练策略参数 $\theta$，也训练 $reward$ 权重$w$。

### 核心思想：策略性能反向驱动 reward 权重学习

虽然我们不知道「什么样的 reward 是对的」，但是强化学习本身会根据 reward 优化策略。

那我们要做的，就是：

> **把 reward 函数的权重 $w$ 也加入到优化中，并让它“配合策略网络”去 jointly maximize 总 reward。**

即：

* 策略网络（Actor）根据当前 reward function 学策略；
* 而 reward function 的加权参数也作为变量，跟策略网络一起训练，让总期望 reward 尽可能大。


### 数学解释：目标函数与联合梯度优化

我们定义 PPO 的期望收益目标为：

$$
J(\pi, w) = \mathbb{E}_{\pi_w}[ \underbrace{w_1 r_1(s,a) + w_2 r_2(s,a)}_{r_w(s,a)} ]
$$

* 其中 $r_1 = \Delta \text{score}$，$r_2 = \text{gain} - \text{loss}$；
* $w = (w_1, w_2)$ 是待学习的参数；
* $\pi_w$ 是在 reward function $r_w$ 下训练出的策略。

然后我们直接用梯度下降去优化这个联合目标：

* 更新策略参数 $\theta$：标准 PPO 更新；
* 更新 reward 权重 $w$：沿着 $\nabla_w J(\pi, w)$ 方向梯度上升。

### 这里要注意的是：

> **reward 的“好坏”不由 ground truth 决定，而是由它是否有利于策略网络学到“高期望回报策略”来决定。**

所以就算 reward 函数本身是错的，训练系统也会找到一个 **“对策略有利”的加权组合**。

### 总结

> **Learnable weighting 是通过“是否对策略有益”来间接衡量 reward function 的质量**，不需要专家轨迹，也不依赖预设权重，只要 reward 函数是连续可导的，就可以与策略一起 end-to-end 优化。

它并不是直接学“对 or 错”的 reward，而是学“有用 or 没用”的 reward 权重。

一个“好的” reward 函数就是那个能指导策略学习高效收敛的 reward 函数。

奖励函数通过引入 learnable weight vector 实现自动调整；

同时对 reward 各分量执行 归一化或比例缩放，避免数值范围差异导致某一分量主导训练；

强化学习使用 PPO 架构，结合 GAE 优化更新效率与稳定性。

## 踩到的一些坑

如果 reward 权重是联合训练的，而训练目标是 maximize reward，那么 reward 参数可能学到“鼓励策略作弊”的方向，从而导致 reward 不再代表我们真正关心的目标。

> 比如说吧，我们希望锚框能尽可能覆盖信号区域，而有不会过多地覆盖噪声区域，
> 这时候我有w1和w2两个参数，w1项是鼓励锚框内的信号功率尽可能高的，w2是鼓励锚框多多扩展面积的，
> 但是在调整奖励函数的时候，模型有可能发现让w2越大越好，这样的话模型只需要无脑扩大锚框就行了，然后越大的w2奖励值也越高，
> 策略很快就收敛了，得到的最终策略更倾向于把锚框区域放大，
> 结果最终得到的锚框完全不是我们想要的，
> 
>  问题根源：reward learning 本质是一个 非平衡博弈
>  - 策略优化目标是尽快最大化 reward；
>  - reward 权重优化目标是让策略 reward 最大； 
>  - 如果没有任何限制，reward 权重会变成策略“最容易作弊”的方向。 
>  
> 所以系统就会掉入「我奖励你放大框 → 你一直放大 → 我继续奖励 → 最终陷入一个 reward hacking 的陷阱」。

另一个问题：

> 上一个举的例子是模型可能会无限放大w2，使得锚框“越大越好”，从而导致模型学到错误的策略，也就是reward hacking现象。
> 但是，假设最开始的模型权重是这样的，它会导致锚框在调整时无脑缩小在信号区域内部，而不是扩展到完整的信号区域上（也就是w1太大了，w2太小了），
> 这种情况模型会不会将错就错学到“让锚框越小越好”这样的结论呢？因为这种情况下的初始权重就是这样的一个趋势，并不是被模型调整成这样的，
> 所以即使使用使用 Softmax 权重或者正则化也无法应对这种问题。
> 
> 这个问题在 meta-RL 里叫做：strategy bias from poor reward initialization.

### 如何跳出strategy bias from poor reward initialization？

我们其实还是要让reward的结构复杂一点。因为现在我们其实只有w1和w2两个权重项，
所以加入更多权重项，比如非凸度惩罚、连通性奖励什么的指标，能更好一些，或者最起码让它不那么容易陷入bias中。

> 跳出 strategy bias from poor reward initialization 的关键，不是靠策略探索能力本身，而是让 reward 函数的结构足够丰富、表达力足够强，使得正确行为能够在 reward 空间中有明确的上升梯度。

# 锚框评分
当前的锚框评分的思路很简单，首先我们需要把频谱做一下归一化，相当于归一化到01之间，噪声接近0，信号高能量区域接近1.
然后计算锚框内的平均值就完事了。
### 锚框评分的另一个思路：高斯混合模型
对整个频谱数据做背景建模（如用高斯混合模型或核密度估计拟合噪声分布）。

对任意锚框内的所有点，计算它们在该噪声模型下的平均负对数似然（即越“异常”负 log-likelihood 越高）。

用这个异常度作为锚框的原始打分：分数越高，说明该区域越像“信号”而非噪声。

（不过目前还没有采用这个，主要是太麻烦了，而且有的时候它似乎不太准。）
# 关于模型

实现了一个基于 PPO（Proximal Policy Optimization） 的锚框微调系统，用于替代原有的 DQN 方法，解决其在训练过程中的不稳定性与收敛困难等问题。

## 模型结构

该系统采用 Actor-Critic PPO 架构，其中状态特征由共享网络提取，具体包括：

SharedFeatureExtractor：对每个区域的输入特征（区域向量 + 锚框）进行编码，使用 Transformer 提取上下文语义；

PPOActor：输出 8 种锚框调整动作的概率分布；

PPOCritic：预测当前状态的价值函数（用于优势估计）。

## 动作空间（Action Space）

共有 8 个离散动作，每个动作表示对锚框的一种微调：

位置调整：上移、下移、左移、右移；

尺寸调整：加宽、变窄、变高、变矮。

## 训练流程（train_agent）

训练基于 PPO 的剪切策略损失函数，使用 GAE（广义优势估计）进行值函数引导，主要流程为：

遍历数据集中的所有锚框区域；

使用 get_feature() 提取状态向量；

调用 select_action() 进行交互，获取动作与状态值；

使用 calculate_reward() 获取每次动作带来的奖励；

存储样本并在每轮结束后使用 update() 进行策略与值函数更新。

每轮训练将打印以下信息：

平均奖励（Avg Reward）

策略网络损失（Actor Loss）

价值网络损失（Critic Loss）

当前时间戳

## 自适应锚框补全机制

这是最后一部分，假如我们生成了50个锚框，但整个频谱存在100条信号迹线，那么即使每个锚框都精确地框选住了一个独立的信号迹线依然会有遗漏的。

接下来，我们需要在之前生成、调整、合并好的锚框基础上，再生成一批锚框（但是新锚框不能落在旧锚框的内部），再对这批锚框进行调整、合并。

这个过程什么时候可以停止呢？我们可以对每次生成的锚框也进行一个统计，比如某一批的锚框评分平均值低于某个值的话，其实就可以停下来了。



